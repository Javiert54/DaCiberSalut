{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Descargar dataset\n",
    "path = kagglehub.dataset_download(\"andrewmvd/lung-and-colon-cancer-histopathological-images\")\n",
    "\n",
    "# Obtener el directorio de trabajo actual\n",
    "destination_path = os.getcwd()+'/dataset/'\n",
    "if not os.path.exists(destination_path):\n",
    "    # Copia la carpeta de forma recursiva\n",
    "    shutil.copytree(path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar imágenes y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "# Abre el archivo JSON\n",
    "with open(\"setup.json\") as archivo:\n",
    "    setup = json.load(archivo)\n",
    "\n",
    "# Accede a los datos\n",
    "print(setup['X_resolution_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset/lung_colon_image_set/lung_image_sets/lung_n: 100%|██████████| 5000/5000 [00:03<00:00, 1482.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset/lung_colon_image_set/colon_image_sets/colon_aca: 100%|██████████| 5000/5000 [00:04<00:00, 1151.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset/lung_colon_image_set/colon_image_sets/colon_n: 100%|██████████| 5000/5000 [00:05<00:00, 934.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset/lung_colon_image_set/lung_image_sets/lung_aca: 100%|██████████| 5000/5000 [00:07<00:00, 701.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando dataset/lung_colon_image_set/lung_image_sets/lung_scc: 100%|██████████| 5000/5000 [00:09<00:00, 528.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Función para procesar y extraer características en lugar de almacenar imágenes completas\n",
    "def process_file(file_path, label, resize=True, X_resolution_size= setup['X_resolution_size'], Y_resolution_size= setup['Y_resolution_size'], color_mode=\"IMREAD_GRAYSCALE\"):\n",
    "    \"\"\"\n",
    "    Procesa un archivo de imagen, lo redimensiona y extrae características.\n",
    "    :param file_path: Ruta del archivo de imagen.\n",
    "    :param label: Etiqueta de la clase.\n",
    "    :param resize: Si se debe redimensionar la imagen.\n",
    "    :param resizeResol: Resolución a la que redimensionar la imagen.\n",
    "    :param color_mode: Modo de color para cargar la imagen (opciones: cv2.IMREAD_COLOR [RGB], IMREAD_GRAYSCALE [Grayscale]).\n",
    "    :return: Un diccionario con características y etiqueta.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        color = cv2.IMREAD_GRAYSCALE  if color_mode == \"IMREAD_GRAYSCALE\" else cv2.IMREAD_COLOR\n",
    "        image = cv2.imread(file_path, color)  # Load in GRAY mode\n",
    "        if resize:\n",
    "            image = cv2.resize(image, (X_resolution_size, Y_resolution_size))  # Resize to X_resolution_size x Y_resolution_size\n",
    "        if image is not None:\n",
    "            return {'features': image.flatten(), 'label': label}  # Store features\n",
    "        else:\n",
    "            print(f\"Error loading image: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception processing {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Generador para manejar archivos sin retener imágenes completas en memoria\n",
    "def image_loader(dirs, labels, max_workers=4):\n",
    "    for dir, label in zip(dirs, labels):\n",
    "        if not os.path.exists(dir):\n",
    "            print(f\"Directorio no encontrado: {dir}\")\n",
    "            continue\n",
    "\n",
    "        files = [os.path.join(dir, file) for file in os.listdir(dir)]\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            print(setup['X_resolution_size'])\n",
    "            results = list(tqdm(executor.map(lambda f: process_file(f, label, True, setup['X_resolution_size'], setup['Y_resolution_size'] ), files), \n",
    "                                desc=f\"Procesando {dir}\", total=len(files)))\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                yield result\n",
    "\n",
    "# Carpeta del dataset\n",
    "dataset_path = 'dataset/lung_colon_image_set/'\n",
    "\n",
    "# Definir las subcarpetas de las clases \n",
    "dirs = [\n",
    "    os.path.join(dataset_path, 'lung_image_sets/lung_n'),\n",
    "    os.path.join(dataset_path, 'colon_image_sets/colon_aca'),\n",
    "    os.path.join(dataset_path, 'colon_image_sets/colon_n'),\n",
    "    os.path.join(dataset_path, 'lung_image_sets/lung_aca'),\n",
    "    os.path.join(dataset_path, 'lung_image_sets/lung_scc')\n",
    "]\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Usar el generador optimizado\n",
    "data = []\n",
    "for result in image_loader(dirs, labels):\n",
    "    data.append(result)\n",
    "\n",
    "# Convertir a un DataFrame más ligero\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = list(df['features'])\n",
    "y = list(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[111, 80, 71, 72, 79, 84, 94, 119, 129, 129, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[191, 190, 190, 192, 195, 195, 198, 196, 190, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[203, 203, 203, 203, 203, 204, 204, 204, 204, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[204, 204, 204, 204, 204, 204, 204, 204, 204, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[202, 200, 199, 199, 199, 201, 201, 201, 201, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label\n",
       "0  [111, 80, 71, 72, 79, 84, 94, 119, 129, 129, 1...      0\n",
       "1  [191, 190, 190, 192, 195, 195, 198, 196, 190, ...      0\n",
       "2  [203, 203, 203, 203, 203, 204, 204, 204, 204, ...      0\n",
       "3  [204, 204, 204, 204, 204, 204, 204, 204, 204, ...      0\n",
       "4  [202, 200, 199, 199, 199, 201, 201, 201, 201, ...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Convertir las etiquetas a categóricas (one-hot encoding) con numpy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = np.eye(\u001b[38;5;28mlen\u001b[39m(labels))[\u001b[43mtrain_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      5\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = np.eye(\u001b[38;5;28mlen\u001b[39m(labels))[val_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      6\u001b[39m test_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = np.eye(\u001b[38;5;28mlen\u001b[39m(labels))[test_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convertir las etiquetas a categóricas (one-hot encoding) con numpy\n",
    "train_df['label'] = np.eye(len(labels))[train_df['label']]\n",
    "val_df['label'] = np.eye(len(labels))[val_df['label']]\n",
    "test_df['label'] = np.eye(len(labels))[test_df['label']]\n",
    "\n",
    "# Mostrar las dimensiones de los conjuntos\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_example(x, X_Resolution_size=512, Y_Resolution_size=512, color_mode=cv2.IMREAD_GRAYSCALE):\n",
    "    \"\"\"\n",
    "    Visualiza un ejemplo de imagen a partir de las características extraídas.\n",
    "    :param x: Características de la imagen.\n",
    "    :param color_mode: Modo de color para visualizar (opciones: cv2.IMREAD_COLOR [RGB], IMREAD_GRAYSCALE [Grayscale]).\n",
    "    \"\"\"\n",
    "    if color_mode == cv2.IMREAD_COLOR:\n",
    "        # Reshape the flattened image back to its original dimensions (e.g., 512x512x3 for RGB)\n",
    "        image = x.reshape(X_Resolution_size, Y_Resolution_size, 3)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct visualization\n",
    "    elif color_mode == cv2.IMREAD_GRAYSCALE:\n",
    "        # Reshape the flattened image back to its original dimensions (e.g., 512x512 for Grayscale)\n",
    "        image = x.reshape(X_Resolution_size, Y_Resolution_size)\n",
    "        plt.imshow(image, cmap='gray')  # Use grayscale colormap\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported color mode. Use cv2.IMREAD_COLOR or cv2.IMREAD_GRAYSCALE.\")\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize the first 5 examples in X_test\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_df[:\u001b[32m5\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mvisualize_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_Resolution_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX_resolution_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Resolution_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mY_resolution_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Print the corresponding label for each image\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mvisualize_example\u001b[39m\u001b[34m(x, X_Resolution_size, Y_Resolution_size, color_mode)\u001b[39m\n\u001b[32m     12\u001b[39m     plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  \u001b[38;5;66;03m# Convert BGR to RGB for correct visualization\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m color_mode == cv2.IMREAD_GRAYSCALE:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Reshape the flattened image back to its original dimensions (e.g., 512x512 for Grayscale)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     image = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m(X_Resolution_size, Y_Resolution_size)\n\u001b[32m     16\u001b[39m     plt.imshow(image, cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Use grayscale colormap\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Visualize the first 5 examples in X_test\n",
    "for i, x in enumerate(test_df[:5]):\n",
    "    visualize_example(x, X_Resolution_size=setup['X_resolution_size'], Y_Resolution_size=['Y_resolution_size'], color_mode=cv2.IMREAD_GRAYSCALE)\n",
    "    print(f\"Label: {y_test[i]}\")  # Print the corresponding label for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "# Adjust input shape based on color mode\n",
    "color_mode = \"RGB\"  # Change to \"Grayscale\" for grayscale images\n",
    "if color_mode == \"RGB\":\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)  # RGB images\n",
    "else:\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 1)  # Grayscale images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Javi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1905152</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │   <span style=\"color: #00af00; text-decoration-color: #00af00\">487,719,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1905152\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │   \u001b[38;5;34m487,719,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m1,542\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">487,813,958</span> (1.82 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m487,813,958\u001b[0m (1.82 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">487,813,958</span> (1.82 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m487,813,958\u001b[0m (1.82 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import layers, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "filters = 32  # Valor de ejemplo para filtros\n",
    "kernel_size = (3, 3)  # Valor de ejemplo para el tamaño del kernel\n",
    "\n",
    "# Ejemplo de una capa convolucional\n",
    "conv_layer = layers.Conv2D(\n",
    "    filters, \n",
    "    kernel_size, \n",
    "    strides=(1, 1), \n",
    "    padding='valid', \n",
    "    data_format=None, \n",
    "    dilation_rate=(1, 1), \n",
    "    activation=None, \n",
    "    use_bias=True, \n",
    "    kernel_initializer='glorot_uniform', \n",
    "    bias_initializer='zeros', \n",
    "    kernel_regularizer=None, \n",
    "    bias_regularizer=None, \n",
    "    activity_regularizer=None, \n",
    "    kernel_constraint=None, \n",
    "    bias_constraint=None    \n",
    ")\n",
    "\n",
    "# Construcción del modelo secuencial\n",
    "IMG_SIZE = 128  # Define el tamaño de las imágenes de entrada\n",
    "model = Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), data_format=\"channels_last\", activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3), kernel_regularizer=l2(0.01)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Entrenamiento del modelo\u001b[39;00m\n\u001b[32m      6\u001b[39m history = model.fit(\n\u001b[32m      7\u001b[39m     train_df[\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m].tolist(),\n\u001b[32m      8\u001b[39m     train_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].tolist(),\n\u001b[32m      9\u001b[39m     epochs=\u001b[32m15\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     validation_data=(val_df[\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m].tolist(), \u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.tolist()),\n\u001b[32m     11\u001b[39m     batch_size=\u001b[32m32\u001b[39m,  \u001b[38;5;66;03m# Especificar el tamaño del batch (opcional)\u001b[39;00m\n\u001b[32m     12\u001b[39m     verbose=\u001b[32m1\u001b[39m       \u001b[38;5;66;03m# Ajustar el nivel de verbosidad (opcional)\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    train_df['features'].tolist(),\n",
    "    train_df['label'].tolist(),\n",
    "    epochs=15,\n",
    "    validation_data=(val_df['features'].tolist(), y_val['label'].tolist()),\n",
    "    batch_size=32,  # Especificar el tamaño del batch (opcional)\n",
    "    verbose=1       # Ajustar el nivel de verbosidad (opcional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos un modelo pre-entrenado con Imagenet\n",
    "vgg_model = VGG19(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "\n",
    "# Congelamos al modelo / que los parámetros no se actualicen\n",
    "for layer in vgg_model.layers[:12]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(vgg_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=500, activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes,   activation=\"softmax\"))\n",
    "model.compile(optimizer='sgd', # Mejor optimizador\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"acc\"])\n",
    "history=model.fit(X_train, y_train, epochs=epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history, title=\"Model Accuracy\"):\n",
    "  \"\"\"Imprime una gráfica mostrando la accuracy por epoch obtenida en un entrenamiento\"\"\"\n",
    "  epochs_range = np.arange(1, len(history.history['acc']) + 1)\n",
    "  plt.plot(epochs_range,history.history['acc'])\n",
    "  plt.plot(epochs_range,history.history['val_acc'])\n",
    "  plt.title(title)\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "def plot_loss(history, title=\"Model Loss\"):\n",
    "  \"\"\"Imprime una gráfica mostrando la pérdida por epoch obtenida en un entrenamiento\"\"\"\n",
    "  epochs_range = np.arange(1, len(history.history['acc']) + 1)\n",
    "  plt.plot(epochs_range,history.history['loss'])\n",
    "  plt.plot(epochs_range,history.history['val_loss'])\n",
    "  plt.title(title)\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Erik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo simple de CNN para clasificación de imágenes\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')  # Número de clases (6)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Extraer métricas del historial\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Número de épocas\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Crear las gráficas\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfica de pérdida (Loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Gráfica de exactitud (Accuracy)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
